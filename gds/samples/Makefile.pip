################################################################################
#
# Copyright 1993-2025 NVIDIA Corporation.  All rights reserved.
#
# NOTICE TO USER:
#
# This source code is subject to NVIDIA ownership rights under U.S. and
# international Copyright laws.
#
# NVIDIA MAKES NO REPRESENTATION ABOUT THE SUITABILITY OF THIS SOURCE
# CODE FOR ANY PURPOSE.  IT IS PROVIDED "AS IS" WITHOUT EXPRESS OR
# IMPLIED WARRANTY OF ANY KIND.  NVIDIA DISCLAIMS ALL WARRANTIES WITH
# REGARD TO THIS SOURCE CODE, INCLUDING ALL IMPLIED WARRANTIES OF
# MERCHANTABILITY, NONINFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE.
# IN NO EVENT SHALL NVIDIA BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL,
# OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
# OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE
# OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE
# OR PERFORMANCE OF THIS SOURCE CODE.
#
# U.S. Government End Users.  This source code is a "commercial item" as
# that term is defined at 48 C.F.R. 2.101 (OCT 1995), consisting  of
# "commercial computer software" and "commercial computer software
# documentation" as such terms are used in 48 C.F.R. 12.212 (SEPT 1995)
# and is provided to the U.S. Government only as a commercial end item.
# Consistent with 48 C.F.R.12.212 and 48 C.F.R. 227.7202-1 through
# 227.7202-4 (JUNE 1995), all U.S. Government End Users acquire the
# source code with only those rights set forth herein.
#
################################################################################
#
# Makefile project only supported on Linux Platforms)
# Updated to use pip-installed CUDA 13.0 libraries
#
################################################################################

# Get the virtual environment path
VENV_PATH := $(shell python -c "import sys; print(sys.prefix)")
CUDA_PIP_PATH := $(VENV_PATH)/lib/python$(shell python -c "import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')")/site-packages/nvidia/cu13

# Common includes and paths for CUDA (using pip-installed libraries)
# Pip packages use unified directory structure across all architectures
ARCH ?= $(shell uname -m)
CUDA_PATH   := $(CUDA_PIP_PATH)
CUFILE_PATH ?= $(CUDA_PIP_PATH)/lib/
CUFILE_INCLUDE_PATH ?= $(CUDA_PIP_PATH)/include/

# Compiler flags
CXXFLAGS    := -Wall
CXXFLAGS    += -I $(CUDA_PATH)/include/ 
CXXFLAGS    += -I $(CUFILE_INCLUDE_PATH)
CXXFLAGS    += -I ./include/

###########################
# Enable the following line for code coverage
ifneq ($(CONFIG_CODE_COVERAGE),)
CXXFLAGS    += -ftest-coverage -fprofile-arcs
endif
CXXFLAGS += -std=c++17
###########################

# Library paths and flags (using pip-installed libraries)
# Note: pip packages only provide versioned libraries (e.g., libcufile.so.0)
# You need to create a symlink: ln -s libcufile.so.0 libcufile.so
CUDART_STATIC := -Bstatic -L $(CUDA_PATH)/lib/ -lcudart_static -lrt -lpthread -ldl
CUFILE_LIB  := -L $(CUFILE_PATH) -lcufile

# Updated LDFLAGS to use pip libraries
LDFLAGS     :=  $(CUFILE_LIB) -L $(CUDA_PATH)/lib/stubs -lcuda $(CUDART_STATIC) -Bdynamic -lrt -ldl

INSTALL_GDSSAMPLES_PREFIX = /usr/local/gds/samples

# Use pip-installed nvcc
NVCC          := $(CUDA_PATH)/bin/nvcc

################################################################################
CC:=/usr/bin/g++
DOCKER_BUILD_REPO=gds_build:manylinux_2_28-cuda-12-2
DOCKERFILE_BUILD_PATH=../../docker/manylinux_2_28-docker/Dockerfile-manylinux_2_28-cuda-12-2

# Target rules
all: build
release_samples:
	docker build -t $(DOCKER_BUILD_REPO) -f $(DOCKERFILE_BUILD_PATH) ./
	docker run -v `pwd`/../../:/nvme-direct:rw,z --rm -it $(DOCKER_BUILD_REPO) sh -c "cd /nvme-direct/tools/samples;$(MAKE) build"
	@echo "Release samples Built"

# Get all .cc files inside common/
common_cc := $(wildcard common/*.cc)
commonobjs := $(patsubst common/%.cc,common/%.o,$(common_cc))
samples = $(filter-out common/%.cc, $(wildcard */*.cc))
cusamples = $(wildcard */*.cu)
objs = $(samples:.cc=)
cuobjs = $(cusamples:.cu=) vectorAdd.o
build: $(commonobjs) $(objs) $(cuobjs)

# Determine NVCC version from pip-installed nvcc
NVCC_VERSION := $(shell $(NVCC) --version 2>/dev/null | grep "release" | sed 's/.*release //' | sed 's/,.*//' || echo "0.0")
NVCC_MAJOR := $(shell echo $(NVCC_VERSION) | cut -d. -f1)
NVCC_MINOR := $(shell echo $(NVCC_VERSION) | cut -d. -f2)

# Set CUDA architectures - matching current Makefile
CUDA_ARCH = -gencode=arch=compute_75,code=sm_75 \
            -gencode=arch=compute_80,code=sm_80 \
            -gencode=arch=compute_86,code=sm_86 \
            -gencode=arch=compute_87,code=sm_87 \
            -gencode=arch=compute_89,code=sm_89 \
            -gencode=arch=compute_90,code=sm_90 \
            -gencode=arch=compute_90,code=compute_90

vectorAdd.o : vectorAdd.cu
	$(NVCC) $(INCLUDES) --cudadevrt static --cudart static \
		$(CUDA_ARCH) --ptxas-options=-v --compiler-options \
		--compile --lib $< -o $@

# Build common object files
common/%.o: common/%.cc
	$(CC) $(INCLUDES) $(CXXFLAGS) -c $< -o $@

# Build rules for all samples (dynamic linking only)
%: %.cc $(CUFILE_INCLUDE_PATH)/cufile.h vectorAdd.o $(commonobjs)
	$(CC) $(INCLUDES) $(CXXFLAGS) $< $(commonobjs) vectorAdd.o -o $@ $(LDFLAGS)

# Special rule for CUDA sample (memmap_thrust)
5_cuFile_MemMap/memmap_thrust: 5_cuFile_MemMap/memmap_thrust.cu $(commonobjs)
	$(NVCC) -I $(CUFILE_INCLUDE_PATH) $(INCLUDES) --cudadevrt static --cudart static $(CUDA_ARCH) $^ -o $@ $(CUFILE_LIB) -lcuda

install:
	cp -r vectorAdd.cu Makefile README.md 1_cuFile_Basics/ 2_cuFile_Multithreaded/ 3_cuFile_Batch/ 4_cuFile_Async/ 5_cuFile_MemMap/ include/ common/ $(INSTALL_GDSSAMPLES_PREFIX)/

clean:
	find . -type f -executable -delete
	rm -f *.o common/*.o cufile.log

# Debug target to show detected paths
debug-paths:
	@echo "=== Virtual Environment ==="
	@echo "VENV_PATH: $(VENV_PATH)"
	@echo ""
	@echo "=== CUDA Pip Paths ==="
	@echo "CUDA_PIP_PATH: $(CUDA_PIP_PATH)"
	@echo "CUDA_PATH: $(CUDA_PATH)"
	@echo "CUFILE_PATH: $(CUFILE_PATH)"
	@echo "CUFILE_INCLUDE_PATH: $(CUFILE_INCLUDE_PATH)"
	@echo ""
	@echo "=== Compiler and Version ==="
	@echo "NVCC: $(NVCC)"
	@echo "NVCC_VERSION: $(NVCC_VERSION)"
	@echo ""
	@echo "=== Architecture ==="
	@echo "ARCH: $(ARCH)"
	@echo "CUDA_ARCH: $(CUDA_ARCH)"
	@echo ""
	@echo "=== Build Targets ==="
	@echo "Common objects: $(commonobjs)"
	@echo "Sample count: $(words $(samples))"
	@echo "CUDA sample count: $(words $(cusamples))"
	@echo ""
	@echo "=== File Checks ==="
	@ls -la $(NVCC) 2>/dev/null || echo "❌ NVCC not found at $(NVCC)"
	@ls -la $(CUFILE_INCLUDE_PATH)/cufile.h 2>/dev/null || echo "❌ cufile.h not found at $(CUFILE_INCLUDE_PATH)/cufile.h"
	@ls -la $(CUFILE_PATH)/libcufile.so 2>/dev/null && echo "✓ libcufile.so symlink exists" || echo "❌ libcufile.so symlink missing - create with: cd $(CUFILE_PATH) && ln -s libcufile.so.0 libcufile.so"
	@ls -la $(CUFILE_PATH)/libcufile.so.0 2>/dev/null && echo "✓ libcufile.so.0 exists" || echo "❌ libcufile.so.0 not found"

.PHONY : build install clean debug-paths

